# TP5 â€” Comprendre la Matrice et Instrumenter lâ€™Environnement (Gymnasium)

## 1) Script random_agent.py
Objectif : initialiser LunarLander-v3, faire agir un agent alÃ©atoire, enregistrer une vidÃ©o (GIF) et calculer des mÃ©triques de vol (score total, usage moteurs, issue).

## 2) GIF de la simulation

![alt text](img/random_agent.gif)

## 3) Rapport de vol (sortie terminal)

![alt text](img/image.png)

## Distance Ã  la rÃ©solution (+200)

Score obtenu par lâ€™agent alÃ©atoire (1 Ã©pisode) : âˆ’57.65 points.

Un agent est considÃ©rÃ© comme â€œrÃ©solvantâ€ lâ€™environnement sâ€™il atteint en moyenne +200 points.

Ã‰cart au seuil :
200 âˆ’ (âˆ’57.65) = 257.65 points.

Lâ€™agent alÃ©atoire est donc trÃ¨s loin de la performance requise. 
Il ne possÃ¨de aucune stratÃ©gie de stabilisation, gaspille du carburant, et finit gÃ©nÃ©ralement par un crash.
Cela confirme que LunarLander nÃ©cessite un apprentissage structurÃ© pour Ãªtre rÃ©solu.

## TP5 â€” PPO (Stable-Baselines3) : EntraÃ®nement + Ã‰valuation

### Ã‰volution de `ep_rew_mean` pendant lâ€™entraÃ®nement
Pendant lâ€™entraÃ®nement PPO (500 000 timesteps), jâ€™ai observÃ© la mÃ©trique `ep_rew_mean` dans les logs :

- Au dÃ©but : `ep_rew_mean` :

![alt text](img/image2.png)

- Ã€ la fin : `ep_rew_mean` : 

![alt text](img/image3.png)

=> `ep_rew_mean` augmente nettement entre le dÃ©but et la fin, ce qui indique que lâ€™agent amÃ©liore sa politique au fil de lâ€™apprentissage.

### GIF de lâ€™agent PPO entraÃ®nÃ©

![alt text](img/trained_ppo_agent.gif)

### Rapport de vol PPO (sortie terminal)

![alt text](img/image1.png)

### Comparaison avec lâ€™agent alÃ©atoire (carburant + issue)
### Comparaison carburant et issue du vol
ğŸ”¹ Agent alÃ©atoire
        Issue : CRASH ğŸ’¥
        Score total : âˆ’57.65
        Allumages moteur principal : 18
        Allumages moteurs latÃ©raux : 31
        DurÃ©e du vol : 64 frames

Lâ€™agent agit sans stratÃ©gie : il consomme du carburant de maniÃ¨re incohÃ©rente et finit rapidement par sâ€™Ã©craser.

ğŸ”¹ Agent PPO entraÃ®nÃ©
        Issue : ATTERRISSAGE RÃ‰USSI ğŸ†
        Score total : 266.30
        Allumages moteur principal : 98
        Allumages moteurs latÃ©raux : 39
        DurÃ©e du vol : 212 frames

Lâ€™agent PPO utilise davantage le moteur principal, mais de maniÃ¨re stratÃ©gique pour contrÃ´ler sa descente et stabiliser lâ€™atterrissage.
Lâ€™utilisation des moteurs latÃ©raux est plus mesurÃ©e et sert au contrÃ´le dâ€™orientation.

### Analyse comparative

Lâ€™agent alÃ©atoire utilise peu de carburant car il crash rapidement.
PPO consomme plus de carburant, mais pour maintenir le contrÃ´le et optimiser la trajectoire.
La durÃ©e de vol est plus longue avec PPO, indiquant un pilotage stabilisÃ©.
La stratÃ©gie PPO maximise la rÃ©compense au lieu dâ€™agir au hasard.

### Seuil de rÃ©solution (+200)

Un agent rÃ©sout lâ€™environnement si le score moyen â‰¥ +200.
Score PPO : 266.30
ep_rew_mean final : â‰ˆ 231
Oui, lâ€™agent PPO dÃ©passe largement le seuil de +200 points et rÃ©sout lâ€™environnement.

## Reward Engineering : Wrapper + Reward Hacking

### Preuve dâ€™exÃ©cution

![alt text](img/image5.png)

![alt text](img/hacked_agent.gif)

### StratÃ©gie observÃ©e (description)

En observant hacked_agent.gif et les mÃ©triques, lâ€™agent adopte une stratÃ©gie visant Ã  Ã©viter lâ€™allumage du moteur principal.
Il privilÃ©gie alors des actions moins pÃ©nalisÃ©es, quitte Ã  perdre le contrÃ´le et Ã  Ã©chouer lâ€™atterrissage.
Le comportement est â€œaberrantâ€ du point de vue humain, mais cohÃ©rent avec la rÃ©compense modifiÃ©e.

### Pourquoi câ€™est optimal (explication math/logique)

Lâ€™agent PPO maximise lâ€™espÃ©rance de la somme des rÃ©compenses :

\[
J(\pi) = \mathbb{E}_{\pi} \left[ \sum_{t=0}^{T} \gamma^t \, r_t \right]
\]

oÃ¹ :

- \( \pi \) est la politique,
- \( r_t \) est la rÃ©compense Ã  lâ€™instant \( t \),
- \( \gamma \in [0,1] \) est le facteur de discount,
- \( T \) est lâ€™horizon de lâ€™Ã©pisode.

Avec notre wrapper, chaque action 2 (moteur principal) ajoute une pÃ©nalitÃ© forte : 
\[
r'_t = r_t - 50 \cdot \mathbf{1}_{\{a_t = 2\}}
\]

Cette pÃ©nalitÃ© domine les termes de rÃ©compense â€œnormauxâ€  et peut rendre toute trajectoire utilisant le moteur principal trÃ¨s dÃ©favorable.
Ainsi, une politique qui Ã©vite lâ€™action 2 peut maximiser 
J(Ï€) mÃªme si elle conduit Ã  un crash, car elle supprime les pertes massives liÃ©es Ã  la pÃ©nalitÃ©.
Câ€™est un exemple de â€œreward hackingâ€ : lâ€™agent optimise exactement la fonction objectif quâ€™on lui donne, pas lâ€™intention rÃ©elle.